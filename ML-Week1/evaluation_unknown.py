# -*- coding: utf-8 -*-
"""evaluation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1--j_Wm7GcJoZI-d7n8OJ3jozEtiADFAr
"""

import pandas as pd
from sklearn.metrics import accuracy_score, classification_report
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score
from sklearn.feature_extraction.text import TfidfVectorizer
# !pip install fastText
import fasttext
import pandas as pd
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer
import string
# ! pip install emoji
import emoji
from sklearn.feature_extraction.text import TfidfVectorizer
import re
# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
import pickle
# from scipy.sparse import hstack

# ! pip install ekphrasis
import ekphrasis
from ekphrasis.classes.preprocessor import TextPreProcessor
hashtag_segmenter = TextPreProcessor(segmenter="twitter", unpack_hashtags=True)


# Load test data
file_name = "./train_split.csv"
train_df = pd.read_csv(file_name)

# Take a look at the data
file_name = "./CL-II-MisinformationData - Sheet1.csv"
test_df = pd.read_csv(file_name)

test_df['label'] = test_df['label'].map({'real': 1, 'fake': 0})

punct_set = set(string.punctuation + '''…'"`’”“'''  + '️')
stoplist = set(nltk.corpus.stopwords.words('english'))

def emoji_split(e, joiner = '\u200d',
                variation_selector=b'\xef\xb8\x8f'.decode('utf-8'),
                return_special_chars = False):
  parts = []
  for part in e:
    if part == joiner:
      if return_special_chars:
        parts.append(":joiner:")
    elif part == variation_selector:
      if return_special_chars:
        parts.append(":variation:")
    else:
      parts.append(part)
  return parts

def data_preprocessor(text):

    # Remove &amp;
    text = text.replace('&amp;', ' ')

    # Remove newline characters
    text = text.replace('\n', ' ')

    # extra space
    text = re.sub('\s+',' ',text)

    # lowercase
    text = text.lower()

    # tokenize
    tt = nltk.tokenize.TweetTokenizer()
    tokens = tt.tokenize(text)

    # Remove stopwords
    tokens = [token for token in tokens if token not in stoplist ]

    # Remove punctuation
    tokens = [token for token in tokens if token not in punct_set]

    # Process tokens
    updated_tokens = []
    lemmatizer = WordNetLemmatizer()

    for t in tokens:
        # Split emoji into components
        if t in emoji.EMOJI_DATA:
            updated_tokens += emoji_split(t)
        # Keep original hashtags and split them into words
        elif t.startswith('#'):
            updated_tokens += hashtag_segmenter.pre_process_doc(t).split(" ")
            # ['<hashtag>']  + hashtag_segmenter.pre_process_doc(t).split(" ") + ['</hashtag>']
        # Replace user mentions with <user>
        elif t.startswith('@'):
            updated_tokens.append('<user>')
        # Replace URLs with <url>
        elif t.startswith('http'):
            updated_tokens.append('<url>')
        else:
            # print(lemmatizer.lemmatize(t))
            updated_tokens.append(lemmatizer.lemmatize(t))

    # de-emojize
    updated_tokens = [emoji.demojize(token) for token in updated_tokens ]

    # Lemmatization
    # lemmatizer = WordNetLemmatizer()
    # updated_tokens = [lemmatizer.lemmatize(token) for token in updated_tokens]

    # print(updated_tokens)
    text = ' '.join(updated_tokens)

    # Replace variations of "co[non-alphanumeric]vid[non-alphanumeric]19" with "covid19"
    text = re.sub(r'co[^\w]*vid', 'covid', text)
    text = re.sub(r'covid[ー^\w]*19', 'covid19', text)

    # clean text
    # text = re.sub(r'\b[^a-zA-Z\s.]+\b', '', text)

    return text

test_df['preprocessed_tweet'] = test_df['tweet'].apply(data_preprocessor)
test_labels = test_df['label']
tfidf_vectorizer = TfidfVectorizer()

tfidf_train_vectors = tfidf_vectorizer.fit_transform(train_df['tweet'])
# Transform the preprocessed tweets into TF-IDF vectors using the trained vectorizer
tfidf_test_vectors = tfidf_vectorizer.transform(test_df['preprocessed_tweet'])

"""# Task 5"""

filename = "knn_model.pickle"
models = ["log_model", "knn_model", "kmean_model", "nn_model", "svm_model"]

for model_file in models:
    # Load the model
    with open(model_file + ".pickle", 'rb') as f:
        loaded_model = pickle.load(f)

    

    # test_labels = test_labels.values.flatten()
    # Make predictions
    test_predictions = loaded_model.predict(tfidf_test_vectors)
    y_true = test_labels

    # Print confusion matrix
    print(f"{model_file} Confusion Matrix:")
    print(confusion_matrix(y_true, test_predictions))
    print("\n")

    # Calculate and print accuracy
    accuracy = accuracy_score(y_true, test_predictions)
    print(f"{model_file} Accuracy: {accuracy:.4f}")

    # Calculate and print precision, recall, and f1-score
    precision = precision_score(y_true, test_predictions, average='weighted')
    recall = recall_score(y_true, test_predictions, average='weighted')
    f1 = f1_score(y_true, test_predictions, average='weighted')

    print(f"{model_file} Precision: {precision:.4f}")
    print(f"{model_file} Recall: {recall:.4f}")
    print(f"{model_file} F1-Score: {f1:.4f}")

    # Print the classification report
    print(f"{model_file} Classification Report:")
    print(classification_report(y_true, test_predictions))
    print("\n")

"""## for fasttext"""

model = fasttext.load_model("model_fasttext.bin")
# test_df=pd.read_csv('./test_split.csv')

test_predictions = [model.predict(text)[0][0] for text in test_df['preprocessed_tweet']]
test_predictions = [1 if label == '__label__real' else 0 for label in test_predictions]

# Print confusion matrix
print(f"fasttext Confusion Matrix:")
print(confusion_matrix(test_labels, test_predictions))
print("\n")

# Calculate and print accuracy
accuracy = accuracy_score(test_labels, test_predictions)
print(f"fasttext Accuracy: {accuracy:.4f}")

# Calculate and print precision, recall, and f1-score
precision = precision_score(test_labels, test_predictions, average='weighted')
recall = recall_score(test_labels, test_predictions, average='weighted')
f1 = f1_score(test_labels, test_predictions, average='weighted')

print(f"fasttext Precision: {precision:.4f}")
print(f"fasttext Recall: {recall:.4f}")
print(f"fasttext F1-Score: {f1:.4f}")

# Print the classification report
print(f"fasttext Classification Report:")
print(classification_report(test_labels, test_predictions))
print("\n")

