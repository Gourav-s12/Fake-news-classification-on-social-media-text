# -*- coding: utf-8 -*-
"""t1_t3_preprocess.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1VoNkWTHNLls3z9wiNTQ8x1mFbyxNKCfX

# Import libaries
"""

import pandas as pd
from sklearn.model_selection import train_test_split
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer
import string
# ! pip install emoji
import emoji
from sklearn.feature_extraction.text import TfidfVectorizer
import re
# Download NLTK resources
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
import pickle
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder
import pickle
# from scipy.sparse import hstack

# ! pip install ekphrasis
import ekphrasis
from ekphrasis.classes.preprocessor import TextPreProcessor
hashtag_segmenter = TextPreProcessor(segmenter="twitter", unpack_hashtags=True)

"""# Load Dataset"""

# Take a look at the data
file_name = "./CL-II-MisinformationData - Sheet1.csv"
df = pd.read_csv(file_name)
df.head(5)

df['label'] = df['label'].map({'real': 1, 'fake': 0})
df.head(5)

label_counts = df['label'].value_counts()
print(label_counts)

punct_set = set(string.punctuation + '''…'"`’”“'''  + '️')
stoplist = set(nltk.corpus.stopwords.words('english'))

def emoji_split(e, joiner = '\u200d',
                variation_selector=b'\xef\xb8\x8f'.decode('utf-8'),
                return_special_chars = False):
  parts = []
  for part in e:
    if part == joiner:
      if return_special_chars:
        parts.append(":joiner:")
    elif part == variation_selector:
      if return_special_chars:
        parts.append(":variation:")
    else:
      parts.append(part)
  return parts

def data_preprocessor(text):

    # Remove &amp;
    text = text.replace('&amp;', ' ')

    # Remove newline characters
    text = text.replace('\n', ' ')

    # extra space
    text = re.sub('\s+',' ',text)

    # lowercase
    text = text.lower()

    # tokenize
    tt = nltk.tokenize.TweetTokenizer()
    tokens = tt.tokenize(text)

    # Remove stopwords
    tokens = [token for token in tokens if token not in stoplist ]

    # Remove punctuation
    tokens = [token for token in tokens if token not in punct_set]

    # Process tokens
    updated_tokens = []
    lemmatizer = WordNetLemmatizer()

    for t in tokens:
        # Split emoji into components
        if t in emoji.EMOJI_DATA:
            updated_tokens += emoji_split(t)
        # Keep original hashtags and split them into words
        elif t.startswith('#'):
            updated_tokens += hashtag_segmenter.pre_process_doc(t).split(" ")
            # ['<hashtag>']  + hashtag_segmenter.pre_process_doc(t).split(" ") + ['</hashtag>']
        # Replace user mentions with <user>
        elif t.startswith('@'):
            updated_tokens.append('<user>')
        # Replace URLs with <url>
        elif t.startswith('http'):
            updated_tokens.append('<url>')
        else:
            # print(lemmatizer.lemmatize(t))
            updated_tokens.append(lemmatizer.lemmatize(t))

    # de-emojize
    updated_tokens = [emoji.demojize(token) for token in updated_tokens ]

    # Lemmatization
    # lemmatizer = WordNetLemmatizer()
    # updated_tokens = [lemmatizer.lemmatize(token) for token in updated_tokens]

    # print(updated_tokens)
    text = ' '.join(updated_tokens)

    # Replace variations of "co[non-alphanumeric]vid[non-alphanumeric]19" with "covid19"
    text = re.sub(r'co[^\w]*vid', 'covid', text)
    text = re.sub(r'covid[ー^\w]*19', 'covid19', text)

    # clean text
    # text = re.sub(r'\b[^a-zA-Z\s.]+\b', '', text)

    return text

df['preprocessed_tweet'] = df['tweet'].apply(data_preprocessor)
df.head(5)

"""# Task 1"""

from sklearn.model_selection import train_test_split
import pandas as pd

X = df['preprocessed_tweet']
y = df['label']

# Split the data into training (80%), validation (10%), and test (10%) sets
X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=True)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42, shuffle=True)

# Create DataFrames for each split
train_df = pd.DataFrame({'tweet': X_train, 'label': y_train})
val_df = pd.DataFrame({'tweet': X_val, 'label': y_val})
test_df = pd.DataFrame({'tweet': X_test, 'label': y_test})

# Save the DataFrames to CSV files
train_df.to_csv('train_split.csv', index=False)
val_df.to_csv('val_split.csv', index=False)
test_df.to_csv('test_split.csv', index=False)

"""# Task 3"""

train_df

tfidf_vectorizer = TfidfVectorizer()

tfidf_train_vectors = tfidf_vectorizer.fit_transform(train_df['tweet'])
tfidf_train_vectors_array = tfidf_train_vectors.toarray()

# Transform test and validation sets using the same vectorizer
tfidf_test_vectors = tfidf_vectorizer.transform(test_df['tweet'])
tfidf_val_vectors = tfidf_vectorizer.transform(val_df['tweet'])

# Save TF-IDF vectors with labels
with open('tfidf_train_vectors_with_labels.pickle', 'wb') as f:
    pickle.dump((tfidf_train_vectors, train_df['label']), f)

with open('tfidf_test_vectors_with_labels.pickle', 'wb') as f:
    pickle.dump((tfidf_test_vectors, test_df['label']), f)

with open('tfidf_val_vectors_with_labels.pickle', 'wb') as f:
    pickle.dump((tfidf_val_vectors, val_df['label']), f)

tfidf_val_vectors.shape