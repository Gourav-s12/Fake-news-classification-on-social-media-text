# -*- coding: utf-8 -*-
"""vectorizer.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lFSpZBtNFjhR_TQyFwE9zQ6zZrUTLm2q
"""

import os
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
import pandas as pd
from transformers import BertTokenizer, BertModel
import numpy as np
from tqdm import tqdm
import pickle
import re
import sys
from transformers import AutoTokenizer, AutoModelForMaskedLM

# tokenizer = AutoTokenizer.from_pretrained("sarkerlab/SocBERT-base")
# model = AutoModelForMaskedLM.from_pretrained("sarkerlab/SocBERT-base")


def process(file_name, model_name):
    # Load pre-trained BERT tokenizer
    print(re.split(r'/', model_name)[1])

    if model_name == "sarkerlab/SocBERT-base" or model_name == "Twitter/twhin-bert-base":
        tokenizer = AutoTokenizer.from_pretrained(model_name)
    else:
        tokenizer = BertTokenizer.from_pretrained(model_name)

    save_model_name = re.split(r'/', model_name)[1] 

    data = pd.read_csv(file_name+"_split.csv")

    text_data = data['tweet']

    # Tokenize input text
    tokenized_text = text_data.apply(lambda x: tokenizer.encode(x, add_special_tokens=True))

    # Pad sequences to the same length
    max_len = max(map(len, tokenized_text))
    padded_tokenized_text = [text + [0]*(max_len-len(text)) for text in tokenized_text]

    # Convert tokenized text to PyTorch tensors
    input_ids = torch.tensor(padded_tokenized_text)

    # Initialize BERT model
    model = BertModel.from_pretrained(model_name)

    # Set the model to evaluation mode
    model.eval()

    # List to store the vectors
    bert_vectors = []

    # Process each text sample
    for text in tqdm(text_data):
        # Tokenize input text
        tokenized_text = tokenizer.encode(text, add_special_tokens=True, max_length=512, truncation=True)

        # Convert tokenized text to PyTorch tensor
        input_ids = torch.tensor(tokenized_text).unsqueeze(0)  # Add batch dimension

        # Forward pass
        with torch.no_grad():
            outputs = model(input_ids)

        # Extract the output representations (vectors) from BERT
        bert_output = outputs[0]  # Output of the last layer

        # Average pooling of the output representations
        pooled_output = torch.mean(bert_output, dim=1).squeeze().numpy()

        # Append the pooled output to the list
        bert_vectors.append(pooled_output)

    # Convert the list of vectors to a numpy array
    bert_vectors = np.array(bert_vectors)

    # print(bert_vectors)

    # bert_vectors.shape

    # Save bert_vectors into pickle file
    with open("bert_vectors_"+file_name+"_"+save_model_name+".pkl", "wb") as f:
        pickle.dump((bert_vectors, data['label']), f)


        
# model_name = "google-bert/bert-base-uncased"
# file_name = "train"
if len(sys.argv) < 2:
    print("please give model name as 1st argument, options are-")
    print("bert-base-uncased   bert-base-cased   covid-twitter-bert   twhin-bert-base   SocBERT-base")
    exit(1)

user_model_name = sys.argv[1]
model_list = ["google-bert/bert-base-uncased", "google-bert/bert-base-cased", "digitalepidemiologylab/covid-twitter-bert","Twitter/twhin-bert-base", "sarkerlab/SocBERT-base"]
file_list = ["val", "train", "test"]

for model_name in model_list:
    if user_model_name == re.split(r'/', model_name)[1]:
        for file_name in file_list:
            print(model_name)
            process(file_name, model_name)
